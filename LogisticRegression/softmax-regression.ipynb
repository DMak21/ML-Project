{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, num_steps=1000,\n",
    "                 regulariser=None,\n",
    "                 lamda=0,\n",
    "                 n_classes=None,\n",
    "                 initial_wts=None):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_steps = num_steps\n",
    "        self.regulariser = regulariser\n",
    "        self.n_classes = n_classes\n",
    "        self.lamda = lamda\n",
    "        self.initial_wts = initial_wts\n",
    "\n",
    "    def _fit(self, X, y, init_params=True):\n",
    "        if init_params:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = np.max(y) + 1\n",
    "            self._n_features = X.shape[1]\n",
    "\n",
    "            \n",
    "            self.b_, self.w_ = self._init_params(\n",
    "                weights_shape=(self._n_features, self.n_classes),\n",
    "                bias_shape=(self.n_classes,))\n",
    "            \n",
    "            if self.initial_wts:\n",
    "                self.b_, self.w_ = np.split(self.initial_wts, [1])\n",
    "                \n",
    "            self.cost_ = []\n",
    "\n",
    "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            for idx in self._yield_batch_idx(\n",
    "                    data=y):\n",
    "                net = self._net_input(X[idx], self.w_, self.b_)\n",
    "                softm = self._softmax(net)\n",
    "                diff = softm - y_enc[idx]\n",
    "                mse = np.mean(diff, axis=0)\n",
    "\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "                \n",
    "                self.w_ -= (self.learning_rate * grad +\n",
    "                            self.learning_rate * self.lamda * self.w_)\n",
    "                self.b_ -= (self.learning_rate * np.sum(diff, axis=0))\n",
    "\n",
    "            net = self._net_input(X, self.w_, self.b_)\n",
    "            softm = self._softmax(net)\n",
    "            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n",
    "            cost = self._cost(cross_ent)\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "        self._fit(X=X, y=y, init_params=init_params)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self._to_classlabels(probas)\n",
    " \n",
    "    def predict(self, X):\n",
    "        return self._predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        net = self._net_input(X, self.w_, self.b_)\n",
    "        softm = self._softmax(net)\n",
    "        return softm\n",
    "\n",
    "    def _net_input(self, X, W, b):\n",
    "        return (X.dot(W) + b)\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def _cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def _cost(self, cross_entropy):\n",
    "        L1_term = self.lamda * np.sum(np.abs(self.w_))\n",
    "        L2_term = self.lamda * np.sum(self.w_ ** 2)\n",
    "        if self.regulariser == 'l1':\n",
    "            cross_entropy = cross_entropy + L1_term\n",
    "        if self.regulariser == 'l2':\n",
    "            cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "    def _to_classlabels(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n",
    "                     scale=1):\n",
    "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(shape=bias_shape)\n",
    "        return b.astype(dtype), w.astype(dtype)\n",
    "    \n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)    \n",
    "    \n",
    "    def _yield_batch_idx(self, data):\n",
    "        indices = np.arange(data.shape[0])\n",
    "        yield indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmak21/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:81: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/dmak21/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(iris.data, iris.target, test_size=.4)\n",
    "\n",
    "logi = SoftmaxRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 2, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 2, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 0, 1, 1,\n",
       "       2, 2, 1, 0, 0, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 2, 1, 0, 0, 0, 2, 2, 1, 1, 0, 1, 2,\n",
       "       1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 2, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 0, 2, 1,\n",
       "       2, 2, 1, 0, 0, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 2, 1, 0, 0, 0, 2, 2, 1, 1, 0, 1, 2,\n",
       "       1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
